
a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. 


|          | name                                                        | Función                               | técnica de cluster       | técnica de cluster                                                                                                                                                                                                                                                                                         | Técnica de representación temática                                                                                                                                                                                        | técnica de cluster                                                                                                                               | Reducción de dimensionalidad                                                |
| -------- | ----------------------------------------------------------- | ------------------------------------- | ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------- |
| ==LDA==  | Latent Dirichlet Allocation                                 | bag-of-words                          |                          |                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                           |                                                                                                                                                  |                                                                             |
| ==NMF==  | Non-Negative Matrix Factorization                           | bag-of-words                          |                          |                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                           |                                                                                                                                                  |                                                                             |
| ==BERT== | Bidirectional Encoder Representations from Transformers<br> | Contextual-words; semantic similarity | centroid based tecniques | density-based perspective (HDBSCAN)<br><br>trabaja con embeddings reducidos<br><br>models clusters<br>using a soft-clustering approach allowing noise<br>to be modeled as outliers. This prevents unrelated<br>documents to be assigned to any cluster and is expected<br>to improve topic representations | Term frequency – Inverse document frequency<br><br>allows for a representation of a term’s<br>importance to a topic instead.<br><br>This allows us to generate<br>topic-word distributions for each cluster of documents. | Sentence-BERT (SBERT) framework<br><br>convert sentences and paragraphs to dense vector<br>representations using pre-trained language models<br> | UMAP to reduce the<br>dimensionality of document embeddings generated<br>in |


##### asumisiones
 + The aforementioned topic modeling techniques assume that words in close proximity to a cluster’s centroid are most representative of that cluster, and thereby a topic 
 + We assume that documents containing the same topic are semantically similar
 + The topic representations are modeled based on the documents in each cluster where each cluster will be assigned one topic
 + we assume that the temporal nature of topics should not influence the creation of global topics. The same topic might appear across different times, albeit possibly represented differently
##### Proceso del modelamiento
1) we first create document embeddings using a pretrained language model to obtain document-level information
2) we first reduce the dimensionality of document embeddings before creating semantically similar clusters of documents that each represent a distinct topic
3) to overcome the centroid-based perspective, we develop a classbased version of TF-IDF to extract the topic representation from each topic

##### BERTopic

BERTopic builds on top of the clustering embeddings approach and extends it by incorporating a class-based variant of TF-IDF for creating topic representations

BERTopic generates topic representations through three steps. First, each document is converted to its embedding representation using a pre-trained language model. Then, before clustering theseembeddings, the dimensionality of the resulting embeddings is reduced to optimize the clustering process. Lastly, from the clusters of documents, topic representations are extracted using a custom class-based variation of TF-IDF.
