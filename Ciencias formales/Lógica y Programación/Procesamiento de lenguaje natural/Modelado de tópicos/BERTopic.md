
a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. 


|          | name                                                        | Función                               | técnica de cluster       | técnica de cluster                                                                                                                                                                                                                                                                                                                                                                            | Técnica de representación temática                                                                                                                                                                                        | técnica de cluster                                                                                                                               | Reducción de dimensionalidad                                                |
| -------- | ----------------------------------------------------------- | ------------------------------------- | ------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------- |
| ==LDA==  | Latent Dirichlet Allocation                                 | bag-of-words                          |                          |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                           |                                                                                                                                                  |                                                                             |
| ==NMF==  | Non-Negative Matrix Factorization                           | bag-of-words                          |                          |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                           |                                                                                                                                                  |                                                                             |
| ==BERT== | Bidirectional Encoder Representations from Transformers<br> | Contextual-words; semantic similarity | centroid based tecniques | density-based perspective (HDBSCAN)<br><br>trabaja con embeddings reducidos<br><br>models clusters<br>using a soft-clustering approach allowing noise<br>to be modeled as outliers. This prevents unrelated<br>documents to be assigned to any cluster and is expected<br>to improve topic representations<br><br>hierarchical density-based spatial<br>clustering of applications with noise | Term frequency – Inverse document frequency<br><br>allows for a representation of a term’s<br>importance to a topic instead.<br><br>This allows us to generate<br>topic-word distributions for each cluster of documents. | Sentence-BERT (SBERT) framework<br><br>convert sentences and paragraphs to dense vector<br>representations using pre-trained language models<br> | UMAP to reduce the<br>dimensionality of document embeddings generated<br>in |


##### asumisiones
 + The aforementioned topic modeling techniques assume that words in close proximity to a cluster’s centroid are most representative of that cluster, and thereby a topic 
 + We assume that documents containing the same topic are semantically similar
 + The topic representations are modeled based on the documents in each cluster where each cluster will be assigned one topic
 + we assume that the temporal nature of topics should not influence the creation of global topics. The same topic might appear across different times, albeit possibly represented differently
 + assumes that each document only contains a single topic which does not reflect the reality that documents may contain multiple topics
 + although BERTopic allows for a contextual representation of documents through its transformer based language models, the topic representation itself does not directly account for that as they are generated from bags-of-words
##### Proceso del modelamiento
1) we first create document embeddings using a pretrained language model to obtain document-level information
2) we first reduce the dimensionality of document embeddings before creating semantically similar clusters of documents that each represent a distinct topic
3) to overcome the centroid-based perspective, we develop a classbased version of TF-IDF to extract the topic representation from each topic

##### BERTopic

BERTopic builds on top of the clustering embeddings approach and extends it by incorporating a class-based variant of TF-IDF for creating topic representations

BERTopic generates topic representations through three steps. First, each document is converted to its embedding representation using a pre-trained language model. Then, before clustering these embeddings, the dimensionality of the resulting embeddings is reduced to optimize the clustering process. Lastly, from the clusters of documents, topic representations are extracted using a custom class-based variation of TF-IDF.

+ This coherence measure has been shown to emulate human judgment with reasonable performance (learns coherent patterns of language)
+ Topic diversity, as defined by (Dieng et al., 2020), is the percentage of unique words for all topics.

We developed BERTopic, a topic model that extends the cluster embedding approach by leveraging state-of-the-art language models and applying a class-based TF-IDF procedure for generating topic representations. By separating the process of clustering documents and generating topic representations, significant flexibility is introduced in the model allowing for ease of usability.